<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="ControlNet is a groundbreaking technology designed to enhance control over image generation processes.n">

    <title>Using ControlNet to Guide Image Generation</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Asap:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

    <link href="assets/fontawesome-pro-6.3.0-web/css/all.min.css" rel="stylesheet">

</head>

<body>
<div class="jumbotron jumbotron-fluid">

    <div>
    <div class="container mt-5">
    <div class="row">

    
        <div class="col-sm-12" style="text-align: left;">
            <h2>Using ControlNet to Guide Image Generation</h2>
            <div class="container-fluid w-85">
            <p class="authors"> 
                ControlNet is a groundbreaking technology designed to enhance control over image generation processes.
            </p>
            </div>
        </div>
        
    </div>
    </div>
    </div>
</div>


<div class="container">
    <div class="section mt-3">
        <div class="container-fluid w-85 mt-2 mb-4">
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <iframe width="80%" height="400px" src="https://www.youtube.com/embed/rCygkyMuSQo?si=5tRhJoA181MwHYy9" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            </div> 
        </div>
        </div>
        <div class="container-fluid w-75">
            <p>
                <span class="bold">ControlNet</span> is an advanced neural network model designed to enhance Stable Diffusion models. It can be integrated with any Stable Diffusion model to provide more precise control over image generation. Stable Diffusion models primarily operate on a text-to-image basis, where text prompts guide the generation of images to match the given descriptions. ControlNet introduces an additional layer of conditioning alongside the text prompts, allowing for more refined control. ControlNet's additional conditioning can take various forms, as demonstrated by the following examples: edge detection and human pose detection.
            </p>
        </div>
    </div>


    <div class="section mt-3">
        <div class="container-fluid w-75">
            <h3>Play With ControlNet</h3>
            <p>
                Play with some cool versions of ContorlNet yourself!
            </p>
        </div>
        <div class="row align-items-center" style="overflow-y: hidden;">
            <div class="col justify-content-center text-center">
                <iframe src="https://hysts-controlnet-v1-1.hf.space" frameborder="0" width="850" height="850" style="overflow-y: hidden;"></iframe>
            </div>
        </div>
    </div>

    <div class="section mt-3">
        <div class="container-fluid w-75">
            <h3>Edge Detection Example</h3>
            <p>
                In the edge detection scenario, ControlNet processes an additional input image to identify its outlines using the Canny edge detector. The detected edges are saved as a control map, which is then used as an extra conditioning input to the ControlNet model alongside the text prompt. This process, referred to as annotation or preprocessing, enables the generation of images that closely follow the detected edges.
            </p>
        
            <div class="container-fluid w-100 mt-2 mb-4">
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="img/img1CannyConditioningSd.png" class="img-fluid">
                </div> 
            </div>
            </div>

        </div>
    </div>

    <div class="section mt-3">
        <div class="container-fluid w-75">
            <h3>Human Pose Detection Example</h3>
            <p>
                Another method of preprocessing involves human pose detection using OpenPose, a model capable of detecting keypoints such as the positions of hands, legs, and head. In this workflow, keypoints are extracted from the input image and saved as a control map. This control map, along with the text prompt, guides the image generation process. The result is an image that adheres to the detected human pose while allowing for creative interpretation.
            </p>
        

            <div class="container-fluid w-100 mt-2 mb-4">
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="img/2OpenPoseHumanPose.png" class="img-fluid">
                </div> 
            </div>
            </div>
        </div>
    </div>

    <div class="section mt-3">
        <div class="container-fluid w-75">
            <h3>ControlNet in Hugging Face Space</h3>
            <p>
            </p>
        </div>

    </div>

    <div class="section mt-3">
        <div class="container-fluid w-75">
            <h3>Example: Human Pose Detection with OpenPose</h3>
            <p>
                Next, we'll upload a photo by Gleb Krasnoborov and apply a new prompt to change the background, effect, and ethnicity of a boxer to Asian. The prompt we use is:
                <br><br><span class="bold">Prompt:</span> "a man shadow boxing in the streets of Tokyo"
            </p>
            <div class="container-fluid w-100 mt-2 mb-4">
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="img/3boxingman.png" class="img-fluid">
                </div> 
            </div>
            </div>
        </div>

    </div>

    
    <div class="section mt-3 mb-3">
        <div class="container-fluid w-75">
            <h3>Scribble Interactive</h3>
            <p>
                ControlNet's architecture can accept various input types. Using Canny edge detection is just one model. There are many more models, each trained for different conditioning in image diffusion. On the same Hugging Face Spaces page, you can access different versions of ControlNet through the top tab. Let's see another example using the Scribbles model. To generate an image using Scribbles, simply go to the Scribble Interactive tab, draw a doodle with your mouse, and write a simple prompt, such as:
                <br><br><span class="bold">Prompt:</span> "a house by the river"

                <br><span class="bold">Using Scribble ControlNet:</span> Drawing a house and providing a text prompt

                <br>Set the other parameters and press the "Run" button to see the output:
            </p>

            <div class="container-fluid w-100 mt-2 mb-4">
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="img/4scribbleHouse.png" class="img-fluid">
                </div> 
            </div>
            </div>

            <p>
                The generated image looks good but could be better. Try adding more details in the scribbles and the text prompt for improved results. Using scribbles and a text prompt is an intuitive way to generate images, especially when you can't think of a precise textual description. Below is another example of creating a picture of a hot air balloon:
            </p>

            <div class="container-fluid w-100 mt-2 mb-4">
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="img/5scribbleBalloon.png" class="img-fluid">
                </div> 
            </div>
            </div>

        </div>

    </div>

    <div class="section mt-3 mb-5">
        <div class="container-fluid w-75">
            <h3>Relevant Resources</h3>
            <a href="https://github.com/lllyasviel/ControlNet" target="_blank">ControlNet on GitHub</a>
        </div>

    </div>



</div>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
<footer>
    <div class="container">
        <p class="tiny"> A page by Story Engine.</p>
    </div>
</footer>

</html>
